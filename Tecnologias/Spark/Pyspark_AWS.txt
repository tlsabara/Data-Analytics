import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job

#meus imports
from datetime import datetime
from pyspark.sql.functions import col, lit

## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
glueContext = GlueContext(SparkContext.getOrCreate())
# sc = SparkContext()
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# -- meu codigo

#
s3_src = "s3://emcash-lake-analytics/db_emcash_prod/usuario_capital/" # tabela capital no S3
s3_diff_dest = "s3://emcash-lake-analytics/glue/spart_tb_1"  # bucket para erros de dados

# Conectando com o catalogo do glue
tb_teste = glueContext.create_dynamic_frame.from_catalog(database="lake_mesh_controladoria", table_name="lm_usuario")

hora_etl = datetime.now()

# test slice
tb_teste = tb_teste.select_fields(paths=["id_usuario", "email", "estagio", "status"])

# teste filtro
# status_campo = "pendente"
status_campo = "ativo"
# tb_teste = tb_teste.filter(lambda x: lit(x["status"]) == lit(status_campo)
# DinDF ta criando um particionamento a mais. --
# Parece que o ETL mesmo vai ser todo no SPARK

# modo classico do Spark --
spark_df = tb_teste.toDF()
spark_df = spark_df.filter(col("status") == status_campo)

# ai adiciona
spark_df = spark_df.withColumn("hora_etl", lit(hora_etl))
spark_df = spark_df.withColumn('rotina_glue', lit('Rotina Spark 1'))

# ai converte em dindf
new_din_df = DynamicFrame.fromDF(spark_df, glueContext, 'new_din_df')
# convertendo novamente para DinDFAWS

# gravando
glueContext.write_dynamic_frame.from_options(
       frame = new_din_df,
       connection_type = "s3",
       connection_options = {"path": s3_diff_dest},
       format = "parquet")
       
# -- fim do meu c√≥digo

job.commit()